{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import yfinance as yf\n",
    "import time\n",
    "from pathlib import Path\n",
    "import os\n",
    "import openpyxl\n",
    "from sec_cik_mapper import StockMapper\n",
    "from datetime import datetime, timedelta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Adam Krupa\\OneDrive\\Pulpit\\Investing\\earnings-report-analysis\n"
     ]
    }
   ],
   "source": [
    "# Get the current working directory\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "# Resolve the parent directories\n",
    "project_general_path = Path(current_dir).resolve()\n",
    "print(project_general_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tickers():\n",
    "    # URL of the website containing the S&P 500 tickers\n",
    "    url = 'https://www.slickcharts.com/sp500'\n",
    "\n",
    "    # Fetch the page content with headers to avoid being blocked\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/85.0.4183.121 Safari/537.36'\n",
    "    }\n",
    "    response = requests.get(url, headers=headers)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        table = soup.find('table', {'class': 'table table-hover table-borderless table-sm'})\n",
    "        tickers = []\n",
    "        \n",
    "        # Extract tickers from the table\n",
    "        if table:\n",
    "            for row in table.find('tbody').find_all('tr'):\n",
    "                columns = row.find_all('td')\n",
    "                ticker = columns[2].text.strip()  # The third column contains the ticker symbol\n",
    "                tickers.append(ticker)\n",
    "        else:\n",
    "            print(\"Table not found in the page\")\n",
    "    else:\n",
    "        print(\"Failed to fetch S&P 500 tickers\")\n",
    "        tickers = []\n",
    "\n",
    "    return tickers\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers = get_tickers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers_short = tickers[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dates(tickers):\n",
    "    # Initialize an empty DataFrame to store valid earnings data\n",
    "    data = pd.DataFrame(columns=['Ticker', 'Date'])\n",
    "\n",
    "    # Loop through each stock ticker\n",
    "    for ticker_symbol in tickers:\n",
    "        ticker = yf.Ticker(ticker_symbol)\n",
    "        # Fetch the earnings history (past earnings reports)\n",
    "        earnings_history = ticker.earnings_dates\n",
    "        \n",
    "        # Skip if no earnings data is available\n",
    "        if earnings_history is None:\n",
    "            print(f\"{ticker_symbol}: possibly delisted; no earnings dates found\")\n",
    "            continue\n",
    "\n",
    "        # Iterate through each earnings report and determine the correct date classification\n",
    "        for index, row in earnings_history.iterrows():\n",
    "            report_time = pd.to_datetime(row.name)  # Access the index (which is the earnings date) and convert it to datetime\n",
    "            # if report_time.hour >= 16:  # After 4 PM, classify as next day because the trade based on this knowledge can effectively only be executed the day after\n",
    "            #     adjusted_date = (report_time + pd.Timedelta(days=1)).date()\n",
    "            # else:  # Otherwise, use the same day\n",
    "            #     adjusted_date = report_time.date()\n",
    "\n",
    "            # Add each adjusted earnings date as a separate row in the DataFrame\n",
    "            new_row = {'Ticker': ticker_symbol, 'Date': report_time}\n",
    "            data = pd.concat([data, pd.DataFrame([new_row])], ignore_index=True)\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cik(data):\n",
    "    mapper = StockMapper()\n",
    "    ticker_to_cik = mapper.ticker_to_cik\n",
    "\n",
    "    # Ensure tickers are in uppercase to match the mapping keys\n",
    "    data['Ticker'] = data['Ticker'].str.upper()\n",
    "\n",
    "    # Map the 'Ticker' column to CIK numbers\n",
    "    data['CIK'] = data['Ticker'].map(ticker_to_cik)\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Adam Krupa\\AppData\\Local\\Temp\\ipykernel_141800\\2737740090.py:26: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  data = pd.concat([data, pd.DataFrame([new_row])], ignore_index=True)\n",
      "BRK.B: $BRK.B: possibly delisted; no earnings dates found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BRK.B: possibly delisted; no earnings dates found\n"
     ]
    }
   ],
   "source": [
    "data = get_dates(tickers_short)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_cik(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking filings for dates: 2024-10-30 and 2024-10-31\n",
      "Form Type: 8-K\n",
      "Filing Date: 2024-10-31\n",
      "Document Link: https://www.sec.gov/Archives/edgar/data/0000320193/aapl-20241031.htm\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "# Replace with your CIK and timestamp of interest\n",
    "cik = '0000320193'  # Example CIK for Apple Inc.\n",
    "date_with_time = '2024-10-30 20:00:00-04:00'  # Timestamp with timezone\n",
    "\n",
    "# Parse the date with timezone information\n",
    "date_time = datetime.fromisoformat(date_with_time)\n",
    "\n",
    "# Derive the two possible dates based on the time\n",
    "date_day_1 = date_time.strftime('%Y-%m-%d')\n",
    "date_day_2 = (date_time + timedelta(days=1)).strftime('%Y-%m-%d')\n",
    "\n",
    "# Ensure the CIK is 10 digits with leading zeros\n",
    "cik_padded = cik.zfill(10)\n",
    "\n",
    "# SEC EDGAR Submissions URL\n",
    "url = f'https://data.sec.gov/submissions/CIK{cik_padded}.json'\n",
    "\n",
    "# Set a custom User-Agent to avoid request blocking\n",
    "headers = {'User-Agent': 'Name Surname (example@gmail.com)'}\n",
    "\n",
    "# Fetch the submissions data\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    data = response.json()\n",
    "    filings = data.get('filings', {}).get('recent', {})\n",
    "    filing_dates = filings.get('filingDate', [])\n",
    "    form_types = filings.get('form', [])\n",
    "    document_links = filings.get('primaryDocument', [])\n",
    "\n",
    "    # Check for filings on both dates\n",
    "    print(f\"Checking filings for dates: {date_day_1} and {date_day_2}\")\n",
    "    for date, form, doc in zip(filing_dates, form_types, document_links):\n",
    "        if date in [date_day_1, date_day_2]:\n",
    "            print(f'Form Type: {form}')\n",
    "            print(f'Filing Date: {date}')\n",
    "            print(f'Document Link: https://www.sec.gov/Archives/edgar/data/{cik_padded}/{doc}')\n",
    "            print('---')\n",
    "else:\n",
    "    print(f'Failed to fetch data: {response.status_code}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Ticker                      Date         CIK\n",
      "0     AAPL 2025-10-28 20:00:00-04:00  0000320193\n",
      "1     AAPL 2025-07-29 20:00:00-04:00  0000320193\n",
      "2     AAPL 2025-04-29 20:00:00-04:00  0000320193\n",
      "3     AAPL 2025-01-29 19:00:00-05:00  0000320193\n",
      "4     AAPL 2024-10-30 20:00:00-04:00  0000320193\n",
      "5     AAPL 2024-07-31 20:00:00-04:00  0000320193\n",
      "6     AAPL 2024-05-01 20:00:00-04:00  0000320193\n",
      "7     AAPL 2024-01-31 19:00:00-05:00  0000320193\n",
      "8     AAPL 2023-11-01 20:00:00-04:00  0000320193\n",
      "9     AAPL 2023-08-02 20:00:00-04:00  0000320193\n",
      "10    AAPL 2023-05-03 20:00:00-04:00  0000320193\n",
      "11    AAPL 2023-02-01 19:00:00-05:00  0000320193\n",
      "12    NVDA 2025-11-18 19:00:00-05:00  0001045810\n",
      "13    NVDA 2025-11-17 19:00:00-05:00  0001045810\n",
      "14    NVDA 2025-08-25 20:00:00-04:00  0001045810\n",
      "15    NVDA 2025-05-19 20:00:00-04:00  0001045810\n",
      "16    NVDA 2025-02-25 19:00:00-05:00  0001045810\n",
      "17    NVDA 2024-11-19 19:00:00-05:00  0001045810\n",
      "18    NVDA 2024-08-27 20:00:00-04:00  0001045810\n",
      "19    NVDA 2024-05-21 20:00:00-04:00  0001045810\n",
      "20    NVDA 2024-02-20 19:00:00-05:00  0001045810\n",
      "21    NVDA 2023-11-20 19:00:00-05:00  0001045810\n",
      "22    NVDA 2023-08-22 20:00:00-04:00  0001045810\n",
      "23    NVDA 2023-05-23 20:00:00-04:00  0001045810\n",
      "24    MSFT 2025-10-27 20:00:00-04:00  0000789019\n",
      "25    MSFT 2025-07-27 20:00:00-04:00  0000789019\n",
      "26    MSFT 2025-04-22 20:00:00-04:00  0000789019\n",
      "27    MSFT 2025-01-27 19:00:00-05:00  0000789019\n",
      "28    MSFT 2024-10-29 20:00:00-04:00  0000789019\n",
      "29    MSFT 2024-07-29 20:00:00-04:00  0000789019\n",
      "30    MSFT 2024-04-24 20:00:00-04:00  0000789019\n",
      "31    MSFT 2024-01-29 19:00:00-05:00  0000789019\n",
      "32    MSFT 2023-10-23 20:00:00-04:00  0000789019\n",
      "33    MSFT 2023-07-24 20:00:00-04:00  0000789019\n",
      "34    MSFT 2023-04-24 20:00:00-04:00  0000789019\n",
      "35    MSFT 2023-01-23 19:00:00-05:00  0000789019\n",
      "36    AMZN 2025-10-28 20:00:00-04:00  0001018724\n",
      "37    AMZN 2025-07-29 20:00:00-04:00  0001018724\n",
      "38    AMZN 2025-04-27 20:00:00-04:00  0001018724\n",
      "39    AMZN 2025-01-29 19:00:00-05:00  0001018724\n",
      "40    AMZN 2024-10-30 20:00:00-04:00  0001018724\n",
      "41    AMZN 2024-07-31 20:00:00-04:00  0001018724\n",
      "42    AMZN 2024-04-29 20:00:00-04:00  0001018724\n",
      "43    AMZN 2024-01-31 19:00:00-05:00  0001018724\n",
      "44    AMZN 2023-10-25 20:00:00-04:00  0001018724\n",
      "45    AMZN 2023-08-02 20:00:00-04:00  0001018724\n",
      "46    AMZN 2023-04-26 20:00:00-04:00  0001018724\n",
      "47    AMZN 2023-02-01 19:00:00-05:00  0001018724\n",
      "48    META 2025-10-27 20:00:00-04:00  0001326801\n",
      "49    META 2025-07-28 20:00:00-04:00  0001326801\n",
      "50    META 2025-04-21 20:00:00-04:00  0001326801\n",
      "51    META 2025-01-29 19:00:00-05:00  0001326801\n",
      "52    META 2024-10-29 20:00:00-04:00  0001326801\n",
      "53    META 2024-07-30 20:00:00-04:00  0001326801\n",
      "54    META 2024-04-23 20:00:00-04:00  0001326801\n",
      "55    META 2024-01-31 19:00:00-05:00  0001326801\n",
      "56    META 2023-10-24 20:00:00-04:00  0001326801\n",
      "57    META 2023-07-25 20:00:00-04:00  0001326801\n",
      "58    META 2023-04-25 20:00:00-04:00  0001326801\n",
      "59    META 2023-01-31 19:00:00-05:00  0001326801\n",
      "60    TSLA 2025-10-20 20:00:00-04:00  0001318605\n",
      "61    TSLA 2025-10-20 20:00:00-04:00  0001318605\n",
      "62    TSLA 2025-07-20 20:00:00-04:00  0001318605\n",
      "63    TSLA 2025-04-22 20:00:00-04:00  0001318605\n",
      "64    TSLA 2025-01-23 19:00:00-05:00  0001318605\n",
      "65    TSLA 2024-10-22 20:00:00-04:00  0001318605\n",
      "66    TSLA 2024-07-22 20:00:00-04:00  0001318605\n",
      "67    TSLA 2024-04-22 20:00:00-04:00  0001318605\n",
      "68    TSLA 2024-01-23 19:00:00-05:00  0001318605\n",
      "69    TSLA 2023-10-17 20:00:00-04:00  0001318605\n",
      "70    TSLA 2023-07-18 20:00:00-04:00  0001318605\n",
      "71    TSLA 2023-04-18 20:00:00-04:00  0001318605\n",
      "72   GOOGL 2025-10-26 20:00:00-04:00  0001652044\n",
      "73   GOOGL 2025-07-20 20:00:00-04:00  0001652044\n",
      "74   GOOGL 2025-04-22 20:00:00-04:00  0001652044\n",
      "75   GOOGL 2025-01-27 19:00:00-05:00  0001652044\n",
      "76   GOOGL 2024-10-28 20:00:00-04:00  0001652044\n",
      "77   GOOGL 2024-07-22 20:00:00-04:00  0001652044\n",
      "78   GOOGL 2024-04-24 20:00:00-04:00  0001652044\n",
      "79   GOOGL 2024-01-29 19:00:00-05:00  0001652044\n",
      "80   GOOGL 2023-10-23 20:00:00-04:00  0001652044\n",
      "81   GOOGL 2023-07-24 20:00:00-04:00  0001652044\n",
      "82   GOOGL 2023-04-24 20:00:00-04:00  0001652044\n",
      "83   GOOGL 2023-02-01 19:00:00-05:00  0001652044\n",
      "84    AVGO 2025-12-09 19:00:00-05:00  0001730168\n",
      "85    AVGO 2025-12-09 19:00:00-05:00  0001730168\n",
      "86    AVGO 2025-09-02 20:00:00-04:00  0001730168\n",
      "87    AVGO 2025-06-09 20:00:00-04:00  0001730168\n",
      "88    AVGO 2025-03-04 19:00:00-05:00  0001730168\n",
      "89    AVGO 2024-12-11 19:00:00-05:00  0001730168\n",
      "90    AVGO 2024-09-04 20:00:00-04:00  0001730168\n",
      "91    AVGO 2024-06-11 20:00:00-04:00  0001730168\n",
      "92    AVGO 2024-03-06 19:00:00-05:00  0001730168\n",
      "93    AVGO 2023-12-06 19:00:00-05:00  0001730168\n",
      "94    AVGO 2023-08-30 20:00:00-04:00  0001730168\n",
      "95    AVGO 2023-05-31 20:00:00-04:00  0001730168\n",
      "96    GOOG 2025-10-26 20:00:00-04:00  0001652044\n",
      "97    GOOG 2025-07-20 20:00:00-04:00  0001652044\n",
      "98    GOOG 2025-04-22 20:00:00-04:00  0001652044\n",
      "99    GOOG 2025-01-27 19:00:00-05:00  0001652044\n",
      "100   GOOG 2024-10-28 20:00:00-04:00  0001652044\n",
      "101   GOOG 2024-07-22 20:00:00-04:00  0001652044\n",
      "102   GOOG 2024-04-24 20:00:00-04:00  0001652044\n",
      "103   GOOG 2024-01-29 19:00:00-05:00  0001652044\n",
      "104   GOOG 2023-10-23 20:00:00-04:00  0001652044\n",
      "105   GOOG 2023-07-24 20:00:00-04:00  0001652044\n",
      "106   GOOG 2023-04-24 20:00:00-04:00  0001652044\n",
      "107   GOOG 2023-02-01 19:00:00-05:00  0001652044\n"
     ]
    }
   ],
   "source": [
    "# Set display options to show all rows and columns\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "print(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "earnings-report-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
